# ğŸ¬ Data Lake de Filmes e SÃ©ries â€“ Desafio TÃ©cnico Compass UOL

> Projeto desenvolvido durante estÃ¡gio na Compass UOL  
> Foco: Engenharia de Dados na AWS

## ğŸ“– Sobre o Projeto

Este repositÃ³rio contÃ©m o desenvolvimento completo do desafio tÃ©cnico realizado durante meu estÃ¡gio na **Compass UOL**.

O desafio foi dividido em **4 sprints**, onde construÃ­ um **Data Lake em AWS**, realizando:

- IngestÃ£o de dados (CSV + API TMDB)
- Processamento e padronizaÃ§Ã£o
- Modelagem dimensional (Star Schema)
- ConstruÃ§Ã£o de Dashboard analÃ­tico no Amazon QuickSight

O projeto simula um cenÃ¡rio real de engenharia de dados, seguindo boas prÃ¡ticas de arquitetura em camadas:

Raw -> Trusted -> Refined

---

# ğŸ—ï¸ Arquitetura do Projeto

ğŸ“Œ Camadas do Data Lake:

- **Raw Layer**
  - Dados brutos (CSV e JSON)
  - Armazenamento no Amazon S3
- **Trusted Layer**
  - Dados limpos e padronizados
  - ConversÃ£o para Parquet
- **Refined Layer**
  - Modelagem dimensional
  - Tabelas Fato e DimensÃ£o
  - Pronto para consumo analÃ­tico

ServiÃ§os AWS utilizados:

- Amazon S3
- AWS Lambda
- AWS Glue (Spark)
- AWS Glue Crawler
- Amazon QuickSight
- Docker

---

# ğŸš€ EvoluÃ§Ã£o por Sprint

Em cada sprint eu resolvi uma etapa do desafio. AlÃ©m disso realizei cursos tÃ©cnicos relacionados para aprofundar meus conhecimentos prÃ¡ticos. PoderÃ¡ ser encontrado um resumo do que aprendi em cada sprint, certificados dos cursos que eu fiz, evidÃªncias do desafio.

## ğŸŸ¢ Sprint 1 â€“ IngestÃ£o de Dados (Raw Layer)

### Objetivo
Criar a camada Raw no S3 realizando ingestÃ£o de:

- Arquivos CSV (movies.csv e series.csv)
- Dados da API do TMDB

### ImplementaÃ§Ã£o

âœ” Upload de CSV para S3 via Python + boto3  
âœ” ContainerizaÃ§Ã£o com Docker  
âœ” IngestÃ£o via AWS Lambda para API TMDB  
âœ” Particionamento por data no S3  

Estrutura de armazenamento:

Raw/<br>
â”œâ”€â”€ Local/CSV/Movies/<br>
â”œâ”€â”€ Local/CSV/Series/<br>
â””â”€â”€ TMDB/JSON/{movies|series}/ano/mes/dia/


---

## ğŸŸ¡ Sprint 2 â€“ Processamento (Trusted Layer)

### Objetivo
Transformar dados brutos em dados tratados no formato Parquet.

### ImplementaÃ§Ã£o

âœ” AWS Glue (Spark)  
âœ” Limpeza de dados  
âœ” DeduplicaÃ§Ã£o  
âœ” Cast de tipos  
âœ” Tratamento de strings  
âœ” ConversÃ£o JSON â†’ Parquet  
âœ” ConversÃ£o CSV â†’ Parquet  

SaÃ­da armazenada na camada:<br>
Trusted/<br>
â”œâ”€â”€ Movies/<br>
â””â”€â”€ Series/


---

## ğŸ”µ Sprint 3 â€“ Modelagem Dimensional (Refined Layer)

### Objetivo
Criar modelo dimensional para anÃ¡lise de dados.

Foram desenvolvidas tabelas:

### ğŸ¬ Para Filmes
- DimFilme
- DimGenero
- DimTempo
- FatoFilme

### ğŸ“º Para SÃ©ries
- DimSerie
- DimGenero
- DimTempo
- FatoSerie

Filtro aplicado:
- Apenas conteÃºdos de **FicÃ§Ã£o CientÃ­fica e Fantasia**

Modelos dimensionais:

![Modelo Filmes](sprint4/Desafio/assets/modeloDimensionalFilmes.png)

![Modelo SÃ©ries](sprint4/Desafio/assets/modeloDimensionalSeries.png)

Dados armazenados em:

Refined/<br>
â”œâ”€â”€ Movies/<br>
â”‚ â”œâ”€â”€ DimFilme/<br>
â”‚ â””â”€â”€ FatoFilme/<br>
â””â”€â”€ Series/<br>
â”œâ”€â”€ DimSerie/<br>
â””â”€â”€ FatoSerie/


âœ” Glue Crawler para catalogaÃ§Ã£o  
âœ” Escrita em Parquet  
âœ” GeraÃ§Ã£o de chaves substitutas  

---

## ğŸŸ£ Sprint 4 â€“ Dashboard (QuickSight)

ConstruÃ§Ã£o de dashboard analÃ­tico para responder perguntas estratÃ©gicas.

### ğŸ“Š Perguntas Analisadas

### ğŸ¬ Filmes

1. Filmes antigos ainda possuem alta popularidade?
2. Existe relaÃ§Ã£o entre nÃºmero de votos e receita?

### ğŸ“º SÃ©ries

1. SÃ©ries mais longas sÃ£o melhor avaliadas?

---

# ğŸ“ˆ Principais Insights

### ğŸ¬ Filmes

- Filmes mais novos tendem a ter maior popularidade mÃ©dia.
- Existe correlaÃ§Ã£o positiva entre nÃºmero de votos e receita.
- Filmes antigos com alta popularidade sÃ£o exceÃ§Ãµes especÃ­ficas.

### ğŸ“º SÃ©ries

- NÃ£o foi identificada correlaÃ§Ã£o clara entre duraÃ§Ã£o total e nota mÃ©dia.

---

# ğŸ“Š Dashboard

Dashboard completo desenvolvido no Amazon QuickSight.

![Dashboard](sprint4/Evidencias/ev3.png)

Principais visualizaÃ§Ãµes:

- Scatter plot (Ano vs Popularidade)
- Scatter plot (Votos vs Receita â€“ escala log)
- Mediana de Receita por faixa de votos
- DuraÃ§Ã£o total vs Nota mÃ©dia

---

# ğŸ¨ Identidade Visual

O dashboard foi desenvolvido utilizando:

- Tema Dark Mode
- Paleta de cores baseada na identidade visual da Compass UOL

---

# ğŸ› ï¸ Tecnologias Utilizadas

- Python 3.10
- Spark (AWS Glue)
- boto3
- AWS Lambda
- Amazon S3
- AWS Glue
- AWS Glue Crawler
- Amazon QuickSight
- Docker

---

# ğŸ“š Aprendizados

Durante as sprints tambÃ©m realizei cursos tÃ©cnicos relacionados a:

- Engenharia de Dados na AWS
- Modelagem Dimensional
- Spark
- Boas prÃ¡ticas em Data Lake

Certificados podem ser encontrados na pasta `/certificados` de cada sprint.

Ao final do estÃ¡gio, consegui obter a certificaÃ§Ã£o AWS cloud practitioner, consolidando meus conhecimentos em AWS. O certificado pode ser encontrado na pasta `/certificados` da Sprint 4.

---

# ğŸ“Œ ConclusÃ£o

Este projeto consolidou conhecimentos prÃ¡ticos em:

âœ” ConstruÃ§Ã£o de Data Lake  
âœ” IngestÃ£o batch e API  
âœ” Processamento distribuÃ­do com Spark  
âœ” Modelagem dimensional  
âœ” ConstruÃ§Ã£o de dashboards analÃ­ticos  
âœ” Arquitetura em camadas (Raw, Trusted, Refined)  

ObservaÃ§Ã£o: nÃ£o pude compartilhar os cÃ³digos de resoluÃ§Ã£o do desafio por questÃµes de confidencialidade, mas coloquei evidÃªncias de execuÃ§Ã£o para demonstrar o processo.